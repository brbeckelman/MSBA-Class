{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jobs = pd.read_csv(\"..\\..\\Assignments\\Assignment1\\Train_rev1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A (Basic Text Mining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenize all job descriptions\n",
    "jobs['tokenized_strip'] = jobs.apply(lambda row: row['FullDescription'].decode('utf-8').strip(),axis=1)\n",
    "jobs['tokenized'] = jobs.apply(lambda row: nltk.word_tokenize(row['tokenized_strip']),axis=1)\n",
    "\n",
    "train = jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get part of speech tags for each token in each description\n",
    "jobs['PoS'] = jobs.apply(lambda row: nltk.pos_tag(row['tokenized']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1. Top 5 Parts of Speech in Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-edaa186ab407>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mall_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokenized'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mall_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_tokens\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokenized'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\brook\\Anaconda2\\lib\\site-packages\\pandas\\core\\series.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    558\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\brook\\Anaconda2\\lib\\site-packages\\pandas\\indexes\\base.pyc\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   1895\u001b[0m         \u001b[1;31m# use this, e.g. DatetimeIndex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1896\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_values'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1897\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1898\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1899\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create list of all tokens in corpus\n",
    "all_tokens = []\n",
    "for i in range(len(train['tokenized'])):\n",
    "    all_tokens = all_tokens + train['tokenized'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get part of speech tags for all tokens in corpus\n",
    "all_PoS = nltk.pos_tag(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens, PoS = zip(*all_PoS) # Unzip all_PoS to separate list of tokens from list of parts of speech\n",
    "PoS_freq = nltk.FreqDist(PoS) # Frequency of each part of speech\n",
    "top5_PoS = PoS_freq.most_common()[:5] # 5 most frequent parts of speech in corpus\n",
    "top5_PoS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 5 most common parts of speech in this corpus along with their respective frequencies are:\n",
    "1. Noun, singular or mass: 46,389\n",
    "2. Proper noun, singular: 33,742\n",
    "3. Preposition or Subordinating Conjunction: 25,724\n",
    "4. Adjective: 22,063\n",
    "5. Determiner: 20,188"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2. Zipf's Law, 100 Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "word_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Get list of words without punctuation for each job description\n",
    "train['tokenized_words'] = train.apply(lambda row: word_tokenizer.tokenize(row['tokenized_strip']),axis=1)\n",
    "\n",
    "# Create list of words from all job descriptions\n",
    "all_words = []\n",
    "for i in range(len(train['tokenized_words'])):\n",
    "    all_words = all_words + train['tokenized_words'][i]\n",
    "\n",
    "# Make all words lowercase\n",
    "words_lower = []\n",
    "for word in all_words:\n",
    "    words_lower.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find 100 most frequent words\n",
    "words_freq = nltk.FreqDist(words_lower)\n",
    "top100_words = words_freq.most_common()[:100]\n",
    "\n",
    "# Get values for expected frequency of top 100 words using Zipf's Law\n",
    "max_freq = top100_words[0][1]\n",
    "counter = 1\n",
    "zipf = []\n",
    "while len(zipf) < 100:\n",
    "    zipf.append(max_freq/counter)\n",
    "    counter += 1\n",
    "\n",
    "# Plot actual frequencies and Zipf frequencies\n",
    "plot([b for (a,b) in top100_words], label = 'Word Frequency') # Blue\n",
    "plot(zipf, label = 'Zipf Predicted Frequency') # Green\n",
    "legend(loc='best')\n",
    "xlabel('Rank')\n",
    "ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 Words after Stemming and Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "no_stops_words = [word for word in words_lower if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "PS = PorterStemmer()\n",
    "\n",
    "# Perform stemming on words\n",
    "stem_words = []\n",
    "for word in no_stops_words:\n",
    "    stem_word = PS.stem(word)\n",
    "    stem_words.append(stem_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find 10 most frequent words\n",
    "words_freq_no_stops = nltk.FreqDist(stem_words)\n",
    "top10_words = words_freq_no_stops.most_common()[:10]\n",
    "\n",
    "top10_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Due to the size of the data set, we chose to use stemming rather than lemmatization. This resulted in a list of the top 10 word stems rather than words themselves or lemmas. However, given the context, the lemmas for each stem can be estimated. \n",
    "\n",
    "The 10 most frequent word stems with their likely associated lemmas are listed below, along with their respective frequencies:\n",
    "1. 'work' (work) - 2141\n",
    "2. 'manag' (manage) - 2049\n",
    "3. 'experi'(experience) - 1642\n",
    "4. 'role' (role) - 1246\n",
    "5. 'team' (team) - 1226\n",
    "6. 'busi' (business) - 1213\n",
    "7. 'client' (client) - 1206\n",
    "8. 'develop' (develop) - 1150\n",
    "9. 'requir' (require) - 1018\n",
    "10. 'servic' (service) - 1017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B (Predict Salary from Job Description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B1. Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = jobs[['FullDescription', 'SalaryNormalized']]\n",
    "\n",
    "#Create new classifier for job posting salaries in the top quartile\n",
    "sal_75 = train['SalaryNormalized'].quantile(.75)\n",
    "train['salary_class'] = np.where(train['SalaryNormalized']>=sal_75,'high','low')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create two separate lists for text and salary classifier for each job posting\n",
    "descriptions = []\n",
    "pay_class = []\n",
    "\n",
    "for index, value in train.iterrows():\n",
    "    descriptions.append(value['FullDescription'])\n",
    "    pay_class.append(value['salary_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split data into test/training set\n",
    "trainset_size = int(round(len(train)*0.75))\n",
    "\n",
    "X_train = np.array([''.join(el) for el in descriptions[0:trainset_size]])\n",
    "y_train = np.array([el for el in pay_class[0:trainset_size]])\n",
    "\n",
    "X_test = np.array([''.join(el) for el in descriptions[trainset_size+1:len(descriptions)]]) \n",
    "y_test = np.array([el for el in pay_class[trainset_size+1:len(pay_class)]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", \n",
    "                             tokenizer= None,\n",
    "                             preprocessor = None, \n",
    "                             stop_words = None)\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Run Naive Bayes Classifier\n",
    "nb_classifier = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "y_nb_predicted = nb_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Run Naive Bayes Classifier\n",
    "nb_classifier = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "y_nb_predicted = nb_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print 'Precision:' + str(metrics.precision_score(y_test, y_nb_predicted, average = 'binary', pos_label='high'))\n",
    "print 'Recall: ' + str(metrics.recall_score(y_test, y_nb_predicted, average = 'binary', pos_label='high'))\n",
    "print 'Accuracy: ' + str(metrics.accuracy_score(y_test, y_nb_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Fancy boy confusion matrix\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "mat = metrics.confusion_matrix(y_test, y_nb_predicted)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=True,\n",
    "            xticklabels=unique(pay_class), yticklabels=unique(pay_class))\n",
    "plt.xlabel('Actual Salary Level')\n",
    "plt.ylabel('Predicted Salary Level');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2: Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe that lemmatization will help improve the accuracy of classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_desc_lem = []\n",
    "for i in train[\"FullDescription\"]:\n",
    "    wrds = i.split()\n",
    "    wrds_lem = []\n",
    "    for j in wrds:\n",
    "        wrds_lem.append(lmtzr.lemmatize(j.decode('utf-8')))\n",
    "    desc_lem = \" \".join(wrds_lem)\n",
    "    all_desc_lem.append(desc_lem)\n",
    "\n",
    "pd.Series(all_desc_lem).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset_size = int(round(len(train)*0.75))\n",
    "\n",
    "X_train = np.array([''.join(el) for el in all_desc_lem[0:trainset_size]])\n",
    "y_train = np.array([el for el in pay_class[0:trainset_size]])\n",
    "\n",
    "X_test = np.array([''.join(el) for el in all_desc_lem[trainset_size+1:len(all_desc_lem)]]) \n",
    "y_test = np.array([el for el in pay_class[trainset_size+1:len(pay_class)]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", \n",
    "                             tokenizer= None,\n",
    "                             preprocessor = None, \n",
    "                             stop_words = None, \n",
    "                             max_features = None)\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run Naive Bayes Classifier\n",
    "nb_classifier = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "y_nb_predicted = nb_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print y_nb_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Precision:' + str(metrics.precision_score(y_test, y_nb_predicted, average = 'binary', pos_label='high'))\n",
    "print 'Recall: ' + str(metrics.recall_score(y_test, y_nb_predicted, average = 'binary', pos_label='high'))\n",
    "print 'Accuracy: ' + str(metrics.accuracy_score(y_test, y_nb_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Fancy boy confusion matrix\n",
    "mat = metrics.confusion_matrix(y_test, y_nb_predicted)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=True,\n",
    "            xticklabels=unique(pay_class), yticklabels=unique(pay_class))\n",
    "plt.xlabel('Actual Salary Level')\n",
    "plt.ylabel('Predicted Salary Level');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B3: Stopwords Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe that removing stopwords will help increase the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset_size = int(round(len(train)*0.75))\n",
    "\n",
    "X_train = np.array([''.join(el) for el in all_desc_lem[0:trainset_size]])\n",
    "y_train = np.array([el for el in pay_class[0:trainset_size]])\n",
    "\n",
    "X_test = np.array([''.join(el) for el in all_desc_lem[trainset_size+1:len(all_desc_lem)]]) \n",
    "y_test = np.array([el for el in pay_class[trainset_size+1:len(pay_class)]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", \n",
    "                             tokenizer= None,\n",
    "                             preprocessor = None, \n",
    "                             stop_words = 'english', \n",
    "                             max_features = None)\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run Naive Bayes Classifier\n",
    "nb_classifier = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "y_nb_predicted = nb_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print y_nb_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Precision:' + str(metrics.precision_score(y_test, y_nb_predicted, average = 'binary', pos_label='high'))\n",
    "print 'Recall: ' + str(metrics.recall_score(y_test, y_nb_predicted, average = 'binary', pos_label='high'))\n",
    "print 'Accuracy: ' + str(metrics.accuracy_score(y_test, y_nb_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Fancy boy confusion matrix\n",
    "mat = metrics.confusion_matrix(y_test, y_nb_predicted)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=True,\n",
    "            xticklabels=unique(pay_class), yticklabels=unique(pay_class))\n",
    "plt.xlabel('Actual Salary Level')\n",
    "plt.ylabel('Predicted Salary Level');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B4. POS Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create two separate lists for text and salary classifier for each job posting\n",
    "descriptions = []\n",
    "pay_class = []\n",
    "\n",
    "for index, value in train.iterrows():\n",
    "    descriptions.append(value['tokenized'])\n",
    "    pay_class.append(value['salary_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_list = []\n",
    "for tok in descriptions:\n",
    "    pos_list.append(nltk.pos_tag(tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#POS appended to word\n",
    "comb = []\n",
    "for pos in pos_list:\n",
    "    to_string = \"\"\n",
    "    for a, b in pos:\n",
    "        to_string += a + \"_\" + b + \" \"\n",
    "    comb.append(to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split data into test/training set\n",
    "trainset_size = int(round(len(train)*0.75))\n",
    "\n",
    "X_train = np.array([''.join(el) for el in comb[0:trainset_size]])\n",
    "y_train = np.array([el for el in pay_class[0:trainset_size]])\n",
    "\n",
    "X_test = np.array([''.join(el) for el in comb[trainset_size+1:len(descriptions)]]) \n",
    "y_test = np.array([el for el in pay_class[trainset_size+1:len(pay_class)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", \n",
    "                             tokenizer= None,\n",
    "                             preprocessor = None, \n",
    "                             stop_words = None, \n",
    "                             max_features = None, ngram_range=(1, 2))\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run Naive Bayes Classifier\n",
    "nb_classifier = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "y_nb_predicted = nb_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Precision:' + str(metrics.precision_score(y_test, y_nb_predicted, average = 'binary', pos_label='high'))\n",
    "print 'Recall: ' + str(metrics.recall_score(y_test, y_nb_predicted, average = 'binary', pos_label='high'))\n",
    "print 'Accuracy: ' + str(metrics.accuracy_score(y_test, y_nb_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Fancy boy confusion matrix\n",
    "mat = metrics.confusion_matrix(y_test, y_nb_predicted)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=True,\n",
    "            xticklabels=unique(pay_class), yticklabels=unique(pay_class))\n",
    "plt.xlabel('Actual Salary Level')\n",
    "plt.ylabel('Predicted Salary Level');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
